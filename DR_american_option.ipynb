{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da5097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# import higherOrderKME\n",
    "# from higherOrderKME import sigkernel\n",
    "from src.path_characteristic_function import char_func_path\n",
    "from src.datasets.data_preparation import prepare_dl\n",
    "from src.train_regressor import train_regressor, plot_reg_losses\n",
    "from src.model import LSTMRegressor, LSTMGenerator, LSTMRegressor_\n",
    "from src.utils import AddTime, to_numpy, track_gradient_norms, track_norm, construct_past_dev_path, construct_future_dev_path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import ml_collections\n",
    "from torch.distributions import Bernoulli\n",
    "import seaborn as sns\n",
    "import os\n",
    "from fbm import FBM\n",
    "from os import path as pt\n",
    "import yaml\n",
    "import ml_collections\n",
    "from torch import nn\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sns.set()\n",
    "torch.manual_seed(0)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574970d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = pt.join(\"configs/configs_DR.yaml\")\n",
    "with open(config_dir) as file:\n",
    "    config = ml_collections.ConfigDict(yaml.safe_load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da261da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('./examples/DR/data_optimal_stopping.obj','rb'))\n",
    "prices, paths = data['prices'], data['paths']\n",
    "\n",
    "# data = torch.load('./examples/Rough/train_X.pt')\n",
    "N, L, D = paths[0].shape\n",
    "paths_torch = np.stack(paths, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d088779e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_train = torch.tensor(paths_torch[:,:-100]).to(dtype=torch.float, device=device)\n",
    "paths_test = torch.tensor(paths_torch[:,-100:]).to(dtype=torch.float, device=device)\n",
    "\n",
    "config.R_input_dim = paths_train.shape[-1]+1\n",
    "config.data_feat_dim = paths_train.shape[-1]\n",
    "config.n_lags = paths_train.shape[2]\n",
    "\n",
    "rank_1_pcf = char_func_path(num_samples=config.Rank_1_num_samples,\n",
    "                           hidden_size=config.Rank_1_lie_degree,\n",
    "                           input_dim=paths_train.shape[-1],\n",
    "                           add_time=True,\n",
    "                           include_initial=False,\n",
    "                           return_sequence=False).to(config.device)\n",
    "\n",
    "rank_1_pcf.load_state_dict(torch.load('./examples/DR/rank_1_pcf.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c55fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_dev_path_trains = []\n",
    "past_dev_path_tests = []\n",
    "for i in range(paths_train.shape[0]):\n",
    "    past_dev_path_train = construct_past_dev_path(rank_1_pcf, AddTime(paths_train[i]).to(device), L)\n",
    "    past_dev_path_test = construct_past_dev_path(rank_1_pcf, AddTime(paths_test[i]).to(device), L)\n",
    "#     print(past_dev_path_train.shape)\n",
    "    past_dev_path_trains.append(past_dev_path_train)\n",
    "    past_dev_path_tests.append(past_dev_path_test)\n",
    "    \n",
    "past_dev_path_trains = torch.stack(past_dev_path_trains)\n",
    "past_dev_path_tests = torch.stack(past_dev_path_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d83c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = []\n",
    "for i in range(paths_train.shape[0]):\n",
    "    regressor = LSTMRegressor(\n",
    "            input_dim=config.R_input_dim,\n",
    "            hidden_dim=config.R_hidden_dim,\n",
    "            output_dim=config.R_output_dim,\n",
    "            n_layers=config.R_num_layers\n",
    "        )\n",
    "    regressor.load_state_dict(torch.load('./examples/DR/regression_{}.pt'.format(i)))\n",
    "    regressor.to(config.device)\n",
    "    regressors.append(regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa899381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expected_dev():\n",
    "\n",
    "    def __init__(self, regressors, lie_degree_1, lie_degree_2, num_samples_2, add_time=True, device='cuda', whole_dev=True):\n",
    "        super(expected_dev, self).__init__()\n",
    "        \"\"\" Generator base class. All generators should be children of this class. \"\"\"\n",
    "        self.device = device\n",
    "        self.regressors = regressors\n",
    "        \n",
    "        for regressor in self.regressors:\n",
    "            regressor.to(device)\n",
    "            regressor.eval()\n",
    "\n",
    "        self.lie_degree_1 = lie_degree_1\n",
    "        self.add_time = add_time\n",
    "        self.num_samples_2 = num_samples_2\n",
    "        self.lie_degree_2 = lie_degree_2\n",
    "        self.pcf_level_2s = defaultdict(list)\n",
    "        \n",
    "        for i in range(24):\n",
    "            for j in range(i,24):\n",
    "                pcf_level_2 = char_func_path(num_samples=self.num_samples_2,\n",
    "                                              hidden_size=self.lie_degree_2,\n",
    "                                              input_dim=2 * self.lie_degree_1 ** 2,\n",
    "                                              add_time=add_time,\n",
    "                                              include_initial=False,\n",
    "                                              return_sequence=False)\n",
    "                pcf_level_2.to(device)\n",
    "                self.pcf_level_2s['{},{}'.format(i,j)] = pcf_level_2\n",
    "        for v in self.pcf_level_2s.values():\n",
    "#             print(v)\n",
    "            v.load_state_dict(self.pcf_level_2s['0,1'].state_dict())\n",
    "            \n",
    "        self.whole_dev = whole_dev\n",
    "        \n",
    "def get_gram_matrix(expected_devx, paths_A, past_dev_path_A, ind_A, paths_B, past_dev_path_B, ind_B):\n",
    "    \n",
    "    N1, M, L, D = paths_A.shape\n",
    "    N2, M, L, D = paths_B.shape\n",
    "\n",
    "    gram_matrix = torch.zeros(N1,N2)\n",
    "    for i in range(N1):\n",
    "        for j in range(i,N2):\n",
    "            expected_devx.pcf_level_2s['{},{}'.format(i,j)].eval()\n",
    "            X = paths_A[i]\n",
    "            Y = paths_B[j]\n",
    "            \n",
    "#             X, past_dev_X = next(iter(paths_train[i]))\n",
    "#             Y, past_dev_Y = next(iter(paths_train[j]))\n",
    "            \n",
    "#             M, L, D = X.shape\n",
    "            \n",
    "            past_dev_X = past_dev_path_A[i]\n",
    "            past_dev_Y = past_dev_path_B[j]\n",
    "            with torch.no_grad():\n",
    "                if expected_devx.whole_dev:\n",
    "                    exp_dev_X = expected_devx.regressors[ind_A[i]](AddTime(X), expected_devx.device)\n",
    "                    exp_dev_Y = expected_devx.regressors[ind_B[j]](AddTime(Y), expected_devx.device)\n",
    "                    exp_dev_X = (past_dev_X @ exp_dev_X).reshape([-1, X.shape[1], expected_devx.lie_degree_1 ** 2])\n",
    "                    exp_dev_Y = (past_dev_Y @ exp_dev_Y).reshape([-1, X.shape[1], expected_devx.lie_degree_1 ** 2])\n",
    "                else:\n",
    "                    exp_dev_X = expected_devx.regressors[ind_A[i]](AddTime(X), expected_devx.device).reshape([-1, X.shape[1], expected_devx.lie_degree_1 ** 2])\n",
    "                    exp_dev_Y = expected_devx.regressors[ind_B[j]](AddTime(Y), expected_devx.device).reshape([-1, X.shape[1], expected_devx.lie_degree_1 ** 2])\n",
    "\n",
    "                exp_dev_X = torch.cat([exp_dev_X.real, exp_dev_X.imag], -1)\n",
    "                exp_dev_Y = torch.cat([exp_dev_Y.real, exp_dev_Y.imag], -1)\n",
    "                \n",
    "                gram_matrix[i,j] = expected_devx.pcf_level_2s['{},{}'.format(i,j)].distance_measure(exp_dev_X, exp_dev_Y, Lambda=0)\n",
    "    return gram_matrix\n",
    "\n",
    "class option_regressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(option_regressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)  # One input feature, 64 hidden units\n",
    "        self.fc2 = nn.Linear(input_dim, 1)   # 64 hidden units, one output feature\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train_option_regressor(gram_matrix, prices, gram_matrix_test, prices_test, num_epochs):\n",
    "    torch.manual_seed(0)\n",
    "    # Instantiate the model\n",
    "    model = option_regressor(gram_matrix.shape[1]).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Forward pass\n",
    "        outputs = model(gram_matrix.to(device))\n",
    "#         loss = ridge_loss(outputs, prices, model, 1)\n",
    "        loss = criterion(outputs, prices)\n",
    "        losses.append(loss.item())\n",
    "#         print(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch% 50 == 0:\n",
    "            model.eval()\n",
    "            criterion = nn.MSELoss()\n",
    "            with torch.no_grad():\n",
    "                pred = model(gram_matrix_test.to(device))\n",
    "                test_loss = criterion(pred, prices_test)\n",
    "#                 print('epoch: ', epoch, 'train_loss: ', loss.item())\n",
    "#                 print('epoch: ', epoch, 'test_loss: ', test_loss.item())\n",
    "#                 print('y_pred: ', pred.T, 'y_test: ', prices_test.T)\n",
    "            model.train()\n",
    "        \n",
    "    return model, losses, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29c0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_torch = torch.tensor(np.array(prices['price'])).to(torch.float).to(device).unsqueeze(0)\n",
    "prices_torch = prices_torch.t() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1f5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train, ind_test, y_train, y_test = train_test_split(np.arange(len(prices_torch)), \n",
    "                                                        to_numpy(prices_torch), \n",
    "                                                        test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fb3d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a687429edf42d181122cc64aaeba0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fa197e44e1436fbea420cd12d47697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b934d6b0ac5b45b69b6c7a50405c3b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_losses = []\n",
    "\n",
    "for seed in range(3):\n",
    "    torch.manual_seed(seed)\n",
    "    expected_devx = expected_dev(regressors = regressors, \n",
    "                                 lie_degree_1 = 5, lie_degree_2 = 8, num_samples_2 = 100, whole_dev=True)\n",
    "    \n",
    "    gram_matrix_whole = get_gram_matrix(expected_devx, \n",
    "                                    paths_train, \n",
    "                                    past_dev_path_trains, \n",
    "                                    list(range(N)), \n",
    "                                    paths_train, \n",
    "                                    past_dev_path_trains, \n",
    "                                    list(range(N)))\n",
    "    gram_matrix_whole_ = gram_matrix_whole + gram_matrix_whole.T\n",
    "    \n",
    "    gram_matrix_train = gram_matrix_whole_[ind_train][:,ind_train]\n",
    "    gram_matrix_test = gram_matrix_whole_[ind_test][:,ind_train]\n",
    "\n",
    "    model, loss, test_loss = train_option_regressor(torch.exp(-2*gram_matrix_train), \n",
    "                                                    torch.tensor(y_train).to(device), \n",
    "                                                    torch.exp(-2*gram_matrix_test), \n",
    "                                                    torch.tensor(y_test).to(device), \n",
    "                                                    1000)\n",
    "    \n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934c9346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00035091652534902096"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(test_losses).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58298570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2990907382336445e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(test_losses).std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7d14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levy",
   "language": "python",
   "name": "levy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
